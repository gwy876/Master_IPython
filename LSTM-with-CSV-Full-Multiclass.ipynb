{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T08:41:40.690455Z",
     "start_time": "2018-07-13T08:41:02.364250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending CICIDS2017\\Friday-WorkingHours-Afternoon-DDos.pcap_ISCX_clean.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python3x64\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (20,21,85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending CICIDS2017\\Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX_clean.csv\n",
      "Appending CICIDS2017\\Friday-WorkingHours-Morning.pcap_ISCX_clean.csv\n",
      "Appending CICIDS2017\\Monday-WorkingHours.pcap_ISCX_clean.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python3x64\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending CICIDS2017\\Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX_clean.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python3x64\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (20,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending CICIDS2017\\Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX_clean.csv\n",
      "Appending CICIDS2017\\Tuesday-WorkingHours.pcap_ISCX_clean.csv\n",
      "Appending CICIDS2017\\Wednesday-workingHours.pcap_ISCX_clean.csv\n",
      "Found these class labels: ['BENIGN' 'DDoS' 'PortScan' 'Bot' 'Infiltration' 'BruteForce' 'XSS'\n",
      " 'SQLInjection' 'FTPPatator' 'SSHPatator' 'DoSSlowloris' 'DoSSlowhttptest'\n",
      " 'DoSHulk' 'DoSGoldenEye' 'Heartbleed']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "flows = pd.DataFrame()\n",
    "\n",
    "for filename in os.listdir('CICIDS2017'): # this is where the CIC CSV files live\n",
    "    if filename.endswith('_clean.csv'):\n",
    "        inputFileName = os.path.join('CICIDS2017', filename)\n",
    "        print('Appending', inputFileName)\n",
    "        new_flows = pd.read_csv(inputFileName)\n",
    "        if 'external_ip' not in new_flows: # This field is not in all datafiles\n",
    "            new_flows['external_ip'] = np.nan\n",
    "\n",
    "        flows = flows.append(new_flows,ignore_index=True,sort=False)\n",
    "\n",
    "print('Found these class labels:', str(flows.label.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of additional, calculated field in the CIC dataset. Whilst these are interesting to have for research purposes, I am mostly interested to stay as close to conventional netflows.  \n",
    "For starters, we'll drop the flow_id as well as the timestamp, as both fields introduce problems and are irrelevant.  \n",
    "As a continuation, we'll drop most of the remaining calculated field to keep the [Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) at bay and keep complexity and training times under control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T08:41:42.953307Z",
     "start_time": "2018-07-13T08:41:40.691501Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_ip</th>\n",
       "      <th>source_port</th>\n",
       "      <th>destination_ip</th>\n",
       "      <th>destination_port</th>\n",
       "      <th>protocol</th>\n",
       "      <th>flow_duration</th>\n",
       "      <th>total_fwd_packets</th>\n",
       "      <th>total_backward_packets</th>\n",
       "      <th>total_length_of_fwd_packets</th>\n",
       "      <th>total_length_of_bwd_packets</th>\n",
       "      <th>flow_bytes_per_s</th>\n",
       "      <th>flow_packets_per_s</th>\n",
       "      <th>fwd_psh_flags</th>\n",
       "      <th>bwd_psh_flags</th>\n",
       "      <th>fwd_urg_flags</th>\n",
       "      <th>bwd_urg_flags</th>\n",
       "      <th>fwd_header_length</th>\n",
       "      <th>bwd_header_length</th>\n",
       "      <th>fwd_packets_per_s</th>\n",
       "      <th>bwd_packets_per_s</th>\n",
       "      <th>fin_flag_count</th>\n",
       "      <th>syn_flag_count</th>\n",
       "      <th>rst_flag_count</th>\n",
       "      <th>psh_flag_count</th>\n",
       "      <th>ack_flag_count</th>\n",
       "      <th>urg_flag_count</th>\n",
       "      <th>cwe_flag_count</th>\n",
       "      <th>ece_flag_count</th>\n",
       "      <th>down_per_up_ratio</th>\n",
       "      <th>fwd_header_length.1</th>\n",
       "      <th>subflow_fwd_packets</th>\n",
       "      <th>subflow_fwd_bytes</th>\n",
       "      <th>subflow_bwd_packets</th>\n",
       "      <th>subflow_bwd_bytes</th>\n",
       "      <th>init_win_bytes_forward</th>\n",
       "      <th>init_win_bytes_backward</th>\n",
       "      <th>act_data_pkt_fwd</th>\n",
       "      <th>label</th>\n",
       "      <th>external_ip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>192.168.10.16</td>\n",
       "      <td>41936</td>\n",
       "      <td>199.244.48.55</td>\n",
       "      <td>443</td>\n",
       "      <td>6</td>\n",
       "      <td>143347</td>\n",
       "      <td>47</td>\n",
       "      <td>60</td>\n",
       "      <td>1325.0</td>\n",
       "      <td>108751.0</td>\n",
       "      <td>767898.8748</td>\n",
       "      <td>746.4404557</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1200</td>\n",
       "      <td>1928</td>\n",
       "      <td>327.875714</td>\n",
       "      <td>418.564742</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1200</td>\n",
       "      <td>47</td>\n",
       "      <td>1325</td>\n",
       "      <td>60</td>\n",
       "      <td>108751</td>\n",
       "      <td>29200</td>\n",
       "      <td>61</td>\n",
       "      <td>30</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>192.168.10.16</td>\n",
       "      <td>42970</td>\n",
       "      <td>54.210.195.63</td>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>50905</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.28887143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>19.644436</td>\n",
       "      <td>19.644436</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>251</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>192.168.10.16</td>\n",
       "      <td>41944</td>\n",
       "      <td>199.244.48.55</td>\n",
       "      <td>443</td>\n",
       "      <td>6</td>\n",
       "      <td>143899</td>\n",
       "      <td>46</td>\n",
       "      <td>58</td>\n",
       "      <td>1325.0</td>\n",
       "      <td>110185.0</td>\n",
       "      <td>774918.5192</td>\n",
       "      <td>722.7291364</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1168</td>\n",
       "      <td>1864</td>\n",
       "      <td>319.668656</td>\n",
       "      <td>403.060480</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1168</td>\n",
       "      <td>46</td>\n",
       "      <td>1325</td>\n",
       "      <td>58</td>\n",
       "      <td>110185</td>\n",
       "      <td>29200</td>\n",
       "      <td>61</td>\n",
       "      <td>30</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>192.168.10.17</td>\n",
       "      <td>12886</td>\n",
       "      <td>192.168.10.3</td>\n",
       "      <td>53</td>\n",
       "      <td>17</td>\n",
       "      <td>313</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>90.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>945686.901</td>\n",
       "      <td>12779.55272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>64</td>\n",
       "      <td>6389.776358</td>\n",
       "      <td>6389.776358</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>206</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>192.168.10.16</td>\n",
       "      <td>41942</td>\n",
       "      <td>199.244.48.55</td>\n",
       "      <td>443</td>\n",
       "      <td>6</td>\n",
       "      <td>142605</td>\n",
       "      <td>45</td>\n",
       "      <td>58</td>\n",
       "      <td>1325.0</td>\n",
       "      <td>108751.0</td>\n",
       "      <td>771894.3936</td>\n",
       "      <td>722.274815</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1136</td>\n",
       "      <td>1864</td>\n",
       "      <td>315.556958</td>\n",
       "      <td>406.717857</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1136</td>\n",
       "      <td>45</td>\n",
       "      <td>1325</td>\n",
       "      <td>58</td>\n",
       "      <td>108751</td>\n",
       "      <td>29200</td>\n",
       "      <td>61</td>\n",
       "      <td>30</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       source_ip  source_port destination_ip  destination_port  protocol  \\\n",
       "0  192.168.10.16        41936  199.244.48.55               443         6   \n",
       "1  192.168.10.16        42970  54.210.195.63                80         6   \n",
       "2  192.168.10.16        41944  199.244.48.55               443         6   \n",
       "3  192.168.10.17        12886   192.168.10.3                53        17   \n",
       "4  192.168.10.16        41942  199.244.48.55               443         6   \n",
       "\n",
       "   flow_duration  total_fwd_packets  total_backward_packets  \\\n",
       "0         143347                 47                      60   \n",
       "1          50905                  1                       1   \n",
       "2         143899                 46                      58   \n",
       "3            313                  2                       2   \n",
       "4         142605                 45                      58   \n",
       "\n",
       "   total_length_of_fwd_packets  total_length_of_bwd_packets flow_bytes_per_s  \\\n",
       "0                       1325.0                     108751.0      767898.8748   \n",
       "1                          0.0                          0.0                0   \n",
       "2                       1325.0                     110185.0      774918.5192   \n",
       "3                         90.0                        206.0       945686.901   \n",
       "4                       1325.0                     108751.0      771894.3936   \n",
       "\n",
       "  flow_packets_per_s  fwd_psh_flags  bwd_psh_flags  fwd_urg_flags  \\\n",
       "0        746.4404557              0              0              0   \n",
       "1        39.28887143              0              0              0   \n",
       "2        722.7291364              0              0              0   \n",
       "3        12779.55272              0              0              0   \n",
       "4         722.274815              0              0              0   \n",
       "\n",
       "   bwd_urg_flags  fwd_header_length  bwd_header_length  fwd_packets_per_s  \\\n",
       "0              0               1200               1928         327.875714   \n",
       "1              0                 32                 32          19.644436   \n",
       "2              0               1168               1864         319.668656   \n",
       "3              0                 40                 64        6389.776358   \n",
       "4              0               1136               1864         315.556958   \n",
       "\n",
       "   bwd_packets_per_s  fin_flag_count  syn_flag_count  rst_flag_count  \\\n",
       "0         418.564742               0               0               0   \n",
       "1          19.644436               0               0               0   \n",
       "2         403.060480               0               0               0   \n",
       "3        6389.776358               0               0               0   \n",
       "4         406.717857               0               0               0   \n",
       "\n",
       "   psh_flag_count  ack_flag_count  urg_flag_count  cwe_flag_count  \\\n",
       "0               1               0               0               0   \n",
       "1               0               1               1               0   \n",
       "2               1               0               0               0   \n",
       "3               0               0               0               0   \n",
       "4               1               0               0               0   \n",
       "\n",
       "   ece_flag_count  down_per_up_ratio  fwd_header_length.1  \\\n",
       "0               0                1.0                 1200   \n",
       "1               0                1.0                   32   \n",
       "2               0                1.0                 1168   \n",
       "3               0                1.0                   40   \n",
       "4               0                1.0                 1136   \n",
       "\n",
       "   subflow_fwd_packets  subflow_fwd_bytes  subflow_bwd_packets  \\\n",
       "0                   47               1325                   60   \n",
       "1                    1                  0                    1   \n",
       "2                   46               1325                   58   \n",
       "3                    2                 90                    2   \n",
       "4                   45               1325                   58   \n",
       "\n",
       "   subflow_bwd_bytes  init_win_bytes_forward  init_win_bytes_backward  \\\n",
       "0             108751                   29200                       61   \n",
       "1                  0                     251                      110   \n",
       "2             110185                   29200                       61   \n",
       "3                206                      -1                       -1   \n",
       "4             108751                   29200                       61   \n",
       "\n",
       "   act_data_pkt_fwd   label external_ip  \n",
       "0                30  BENIGN         NaN  \n",
       "1                 0  BENIGN         NaN  \n",
       "2                30  BENIGN         NaN  \n",
       "3                 1  BENIGN         NaN  \n",
       "4                30  BENIGN         NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop unused fields.\n",
    "unused_fields = ['flow_id','timestamp','fwd_packet_length_max','fwd_packet_length_min','fwd_packet_length_mean','fwd_packet_length_std','bwd_packet_length_max','bwd_packet_length_min','bwd_packet_length_mean','bwd_packet_length_std','flow_iat_mean','flow_iat_std','flow_iat_max','flow_iat_min','fwd_iat_total','fwd_iat_mean','fwd_iat_std','fwd_iat_max','fwd_iat_min','bwd_iat_total','bwd_iat_mean','bwd_iat_std','bwd_iat_max','bwd_iat_min','min_packet_length','max_packet_length','packet_length_mean','packet_length_std','packet_length_variance','average_packet_size','avg_fwd_segment_size','avg_bwd_segment_size','fwd_avg_bytes_per_bulk','fwd_avg_packets_per_bulk','fwd_avg_bulk_rate','bwd_avg_bytes_per_bulk','bwd_avg_packets_per_bulk','bwd_avg_bulk_rate','active_mean','active_std','active_max','active_min','idle_mean','idle_std','idle_max','idle_min','min_seg_size_forward']\n",
    "flows.drop(unused_fields, axis=1, inplace=True)\n",
    "flows.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's still a problem: How can we encode IP addresses in a way that the neural network can make use of them while preserving the hierarchical information they contain?  \n",
    "Encoding IPs through One Hot let's comlexity and training times explode, so for now I am splitting each IP into its four octet pairs and interpret them as numbers.  \n",
    "Maybe there's a better way to represent them (especially because I am only able to encode IPv4 right now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T08:42:02.667427Z",
     "start_time": "2018-07-13T08:41:42.954279Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_port</th>\n",
       "      <th>destination_port</th>\n",
       "      <th>protocol</th>\n",
       "      <th>flow_duration</th>\n",
       "      <th>total_fwd_packets</th>\n",
       "      <th>total_backward_packets</th>\n",
       "      <th>total_length_of_fwd_packets</th>\n",
       "      <th>total_length_of_bwd_packets</th>\n",
       "      <th>flow_bytes_per_s</th>\n",
       "      <th>flow_packets_per_s</th>\n",
       "      <th>fwd_psh_flags</th>\n",
       "      <th>bwd_psh_flags</th>\n",
       "      <th>fwd_urg_flags</th>\n",
       "      <th>bwd_urg_flags</th>\n",
       "      <th>fwd_header_length</th>\n",
       "      <th>bwd_header_length</th>\n",
       "      <th>fwd_packets_per_s</th>\n",
       "      <th>bwd_packets_per_s</th>\n",
       "      <th>fin_flag_count</th>\n",
       "      <th>syn_flag_count</th>\n",
       "      <th>rst_flag_count</th>\n",
       "      <th>psh_flag_count</th>\n",
       "      <th>ack_flag_count</th>\n",
       "      <th>urg_flag_count</th>\n",
       "      <th>cwe_flag_count</th>\n",
       "      <th>ece_flag_count</th>\n",
       "      <th>down_per_up_ratio</th>\n",
       "      <th>fwd_header_length.1</th>\n",
       "      <th>subflow_fwd_packets</th>\n",
       "      <th>subflow_fwd_bytes</th>\n",
       "      <th>subflow_bwd_packets</th>\n",
       "      <th>subflow_bwd_bytes</th>\n",
       "      <th>init_win_bytes_forward</th>\n",
       "      <th>init_win_bytes_backward</th>\n",
       "      <th>act_data_pkt_fwd</th>\n",
       "      <th>label</th>\n",
       "      <th>source_ip_o1</th>\n",
       "      <th>source_ip_o2</th>\n",
       "      <th>source_ip_o3</th>\n",
       "      <th>source_ip_o4</th>\n",
       "      <th>destination_ip_o1</th>\n",
       "      <th>destination_ip_o2</th>\n",
       "      <th>destination_ip_o3</th>\n",
       "      <th>destination_ip_o4</th>\n",
       "      <th>external_ip_o1</th>\n",
       "      <th>external_ip_o2</th>\n",
       "      <th>external_ip_o3</th>\n",
       "      <th>external_ip_o4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41936</td>\n",
       "      <td>443</td>\n",
       "      <td>6</td>\n",
       "      <td>143347</td>\n",
       "      <td>47</td>\n",
       "      <td>60</td>\n",
       "      <td>1325.0</td>\n",
       "      <td>108751.0</td>\n",
       "      <td>767898.8748</td>\n",
       "      <td>746.4404557</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1200</td>\n",
       "      <td>1928</td>\n",
       "      <td>327.875714</td>\n",
       "      <td>418.564742</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1200</td>\n",
       "      <td>47</td>\n",
       "      <td>1325</td>\n",
       "      <td>60</td>\n",
       "      <td>108751</td>\n",
       "      <td>29200</td>\n",
       "      <td>61</td>\n",
       "      <td>30</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>192</td>\n",
       "      <td>168</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>199</td>\n",
       "      <td>244</td>\n",
       "      <td>48</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42970</td>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>50905</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.28887143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>19.644436</td>\n",
       "      <td>19.644436</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>251</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>192</td>\n",
       "      <td>168</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>54</td>\n",
       "      <td>210</td>\n",
       "      <td>195</td>\n",
       "      <td>63</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41944</td>\n",
       "      <td>443</td>\n",
       "      <td>6</td>\n",
       "      <td>143899</td>\n",
       "      <td>46</td>\n",
       "      <td>58</td>\n",
       "      <td>1325.0</td>\n",
       "      <td>110185.0</td>\n",
       "      <td>774918.5192</td>\n",
       "      <td>722.7291364</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1168</td>\n",
       "      <td>1864</td>\n",
       "      <td>319.668656</td>\n",
       "      <td>403.060480</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1168</td>\n",
       "      <td>46</td>\n",
       "      <td>1325</td>\n",
       "      <td>58</td>\n",
       "      <td>110185</td>\n",
       "      <td>29200</td>\n",
       "      <td>61</td>\n",
       "      <td>30</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>192</td>\n",
       "      <td>168</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>199</td>\n",
       "      <td>244</td>\n",
       "      <td>48</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12886</td>\n",
       "      <td>53</td>\n",
       "      <td>17</td>\n",
       "      <td>313</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>90.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>945686.901</td>\n",
       "      <td>12779.55272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>64</td>\n",
       "      <td>6389.776358</td>\n",
       "      <td>6389.776358</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>206</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>192</td>\n",
       "      <td>168</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>192</td>\n",
       "      <td>168</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41942</td>\n",
       "      <td>443</td>\n",
       "      <td>6</td>\n",
       "      <td>142605</td>\n",
       "      <td>45</td>\n",
       "      <td>58</td>\n",
       "      <td>1325.0</td>\n",
       "      <td>108751.0</td>\n",
       "      <td>771894.3936</td>\n",
       "      <td>722.274815</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1136</td>\n",
       "      <td>1864</td>\n",
       "      <td>315.556958</td>\n",
       "      <td>406.717857</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1136</td>\n",
       "      <td>45</td>\n",
       "      <td>1325</td>\n",
       "      <td>58</td>\n",
       "      <td>108751</td>\n",
       "      <td>29200</td>\n",
       "      <td>61</td>\n",
       "      <td>30</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>192</td>\n",
       "      <td>168</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>199</td>\n",
       "      <td>244</td>\n",
       "      <td>48</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source_port  destination_port  protocol  flow_duration  total_fwd_packets  \\\n",
       "0        41936               443         6         143347                 47   \n",
       "1        42970                80         6          50905                  1   \n",
       "2        41944               443         6         143899                 46   \n",
       "3        12886                53        17            313                  2   \n",
       "4        41942               443         6         142605                 45   \n",
       "\n",
       "   total_backward_packets  total_length_of_fwd_packets  \\\n",
       "0                      60                       1325.0   \n",
       "1                       1                          0.0   \n",
       "2                      58                       1325.0   \n",
       "3                       2                         90.0   \n",
       "4                      58                       1325.0   \n",
       "\n",
       "   total_length_of_bwd_packets flow_bytes_per_s flow_packets_per_s  \\\n",
       "0                     108751.0      767898.8748        746.4404557   \n",
       "1                          0.0                0        39.28887143   \n",
       "2                     110185.0      774918.5192        722.7291364   \n",
       "3                        206.0       945686.901        12779.55272   \n",
       "4                     108751.0      771894.3936         722.274815   \n",
       "\n",
       "   fwd_psh_flags  bwd_psh_flags  fwd_urg_flags  bwd_urg_flags  \\\n",
       "0              0              0              0              0   \n",
       "1              0              0              0              0   \n",
       "2              0              0              0              0   \n",
       "3              0              0              0              0   \n",
       "4              0              0              0              0   \n",
       "\n",
       "   fwd_header_length  bwd_header_length  fwd_packets_per_s  bwd_packets_per_s  \\\n",
       "0               1200               1928         327.875714         418.564742   \n",
       "1                 32                 32          19.644436          19.644436   \n",
       "2               1168               1864         319.668656         403.060480   \n",
       "3                 40                 64        6389.776358        6389.776358   \n",
       "4               1136               1864         315.556958         406.717857   \n",
       "\n",
       "   fin_flag_count  syn_flag_count  rst_flag_count  psh_flag_count  \\\n",
       "0               0               0               0               1   \n",
       "1               0               0               0               0   \n",
       "2               0               0               0               1   \n",
       "3               0               0               0               0   \n",
       "4               0               0               0               1   \n",
       "\n",
       "   ack_flag_count  urg_flag_count  cwe_flag_count  ece_flag_count  \\\n",
       "0               0               0               0               0   \n",
       "1               1               1               0               0   \n",
       "2               0               0               0               0   \n",
       "3               0               0               0               0   \n",
       "4               0               0               0               0   \n",
       "\n",
       "   down_per_up_ratio  fwd_header_length.1  subflow_fwd_packets  \\\n",
       "0                1.0                 1200                   47   \n",
       "1                1.0                   32                    1   \n",
       "2                1.0                 1168                   46   \n",
       "3                1.0                   40                    2   \n",
       "4                1.0                 1136                   45   \n",
       "\n",
       "   subflow_fwd_bytes  subflow_bwd_packets  subflow_bwd_bytes  \\\n",
       "0               1325                   60             108751   \n",
       "1                  0                    1                  0   \n",
       "2               1325                   58             110185   \n",
       "3                 90                    2                206   \n",
       "4               1325                   58             108751   \n",
       "\n",
       "   init_win_bytes_forward  init_win_bytes_backward  act_data_pkt_fwd   label  \\\n",
       "0                   29200                       61                30  BENIGN   \n",
       "1                     251                      110                 0  BENIGN   \n",
       "2                   29200                       61                30  BENIGN   \n",
       "3                      -1                       -1                 1  BENIGN   \n",
       "4                   29200                       61                30  BENIGN   \n",
       "\n",
       "  source_ip_o1 source_ip_o2 source_ip_o3 source_ip_o4 destination_ip_o1  \\\n",
       "0          192          168           10           16               199   \n",
       "1          192          168           10           16                54   \n",
       "2          192          168           10           16               199   \n",
       "3          192          168           10           17               192   \n",
       "4          192          168           10           16               199   \n",
       "\n",
       "  destination_ip_o2 destination_ip_o3 destination_ip_o4 external_ip_o1  \\\n",
       "0               244                48                55            NaN   \n",
       "1               210               195                63            NaN   \n",
       "2               244                48                55            NaN   \n",
       "3               168                10                 3            NaN   \n",
       "4               244                48                55            NaN   \n",
       "\n",
       "  external_ip_o2 external_ip_o3 external_ip_o4  \n",
       "0            NaN            NaN            NaN  \n",
       "1            NaN            NaN            NaN  \n",
       "2            NaN            NaN            NaN  \n",
       "3            NaN            NaN            NaN  \n",
       "4            NaN            NaN            NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/14745022/how-to-split-a-column-into-two-columns\n",
    "# FIXME: Right now, only IPv4 (4 octets)\n",
    "\n",
    "# Split the String representation of the IP into it's four octects, which are delimited by a dot\n",
    "flows['source_ip_o1'],flows['source_ip_o2'],flows['source_ip_o3'],flows['source_ip_o4'] = flows['source_ip'].str.split('.').str\n",
    "flows['destination_ip_o1'],flows['destination_ip_o2'],flows['destination_ip_o3'],flows['destination_ip_o4'] = flows['destination_ip'].str.split('.').str\n",
    "flows['external_ip_o1'],flows['external_ip_o2'],flows['external_ip_o3'],flows['external_ip_o4'] = flows['external_ip'].str.split('.').str\n",
    "\n",
    "# After completion, drop the initial columns, as they aren't needed anymore\n",
    "flows.drop(['source_ip'], axis=1, inplace=True)\n",
    "flows.drop(['destination_ip'], axis=1, inplace=True)\n",
    "flows.drop(['external_ip'], axis=1, inplace=True)\n",
    "\n",
    "# Finally, let's inspect the outcome\n",
    "flows.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels of the dataset (as in: *Benign*, *DDoS*, *Portscan*, etc) are converted into a list of integers and split off of the main DataFrame.  \n",
    "After this step there is a variable `enc_labels` that holds an integer-encoded list of labels.\n",
    "A humble example (not representative):  \n",
    "\n",
    "|Label         | Value          |\n",
    "|------------- |---------:|\n",
    "|Benign      | 0|\n",
    "|DDoS        | 1|\n",
    "|Portscan    | 2|  \n",
    "\n",
    "So if the order of the first three Netflows would be *Benign*, *Benign*, *DDos*,  \n",
    "the resulting `enc_labels` would look like this: `[1,1,2]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T08:42:19.022094Z",
     "start_time": "2018-07-13T08:42:02.668425Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# tokenize the LABELS\n",
    "label_tokenizer = Tokenizer(num_words=20, filters='') # don't filter any of the characters. 1 entry = 1 label \n",
    "label_tokenizer.fit_on_texts(flows['label'])\n",
    "\n",
    "# Run the fitted tokenizer on the label column and save the encoded data as dataframe\n",
    "enc_labels = label_tokenizer.texts_to_sequences(flows['label'])\n",
    "enc_labels = np.concatenate(enc_labels).ravel()\n",
    "\n",
    "# as the Encoder documentation states, 0 will never assigned to a label.\n",
    "# I, on the other hand, need an index starting with 0. So we substract 1 of all classes.\n",
    "enc_labels = enc_labels -1\n",
    "\n",
    "# finally, drop the label column\n",
    "flows.drop(['label'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're at it, we make sure to never have any float values exceeding +/- infinity as well as NaN values.  \n",
    "These are all replaces by zeros, which is a temporary fix and definitely a FIXME for the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T08:42:39.834377Z",
     "start_time": "2018-07-13T08:42:19.023065Z"
    }
   },
   "outputs": [],
   "source": [
    "# weed out all NaN and infinite values\n",
    "flows.replace([np.inf, -np.inf], np.nan)\n",
    "flows.fillna(inplace=True, value=0) # FIXME: 0 for now, find a better way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Keras seems to be a bit picky about the presented datatypes, we'll convert the Pandas DataFrame into it's underlying representation of Numpy-Arrays and work with these from this point onwars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T08:42:44.198242Z",
     "start_time": "2018-07-13T08:42:39.835374Z"
    }
   },
   "outputs": [],
   "source": [
    "flows_nd = flows.astype('float64').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T08:42:44.498789Z",
     "start_time": "2018-07-13T08:42:44.199213Z"
    }
   },
   "outputs": [],
   "source": [
    "# as the pandas infinity stuff is seemingly not enough, check the numpy array once again\n",
    "from numpy import inf\n",
    "flows_nd[flows_nd == -inf] = 0\n",
    "flows_nd[flows_nd == inf] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T08:42:44.711705Z",
     "start_time": "2018-07-13T08:42:44.499787Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has NaN: False\n",
      "Data has only finite values True\n"
     ]
    }
   ],
   "source": [
    "print(\"Data has NaN:\",np.any(np.isnan(flows_nd)))\n",
    "print(\"Data has only finite values\",np.all(np.isfinite(flows_nd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization and Finishing Touches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best performance is achieved if all values are normalized. In this approach I am using [sklearn's MinMaxScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html), which implements feature scaling through MinMax-Normalization (Rescaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T08:42:46.918146Z",
     "start_time": "2018-07-13T08:42:44.713656Z"
    }
   },
   "outputs": [],
   "source": [
    "# FIXME: Don't do normalization on test data!\n",
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "flows_scaled = min_max_scaler.fit_transform(flows_nd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a final glance at a single entry of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T08:42:46.924022Z",
     "start_time": "2018-07-13T08:42:46.919034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.39902342e-01 6.75974670e-03 3.52941176e-01 1.19466656e-03\n",
      " 2.09321162e-04 2.05534355e-04 1.02713178e-04 1.65917305e-04\n",
      " 1.12250385e-01 3.33457740e-01 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 9.99855861e-01 9.94593719e-01\n",
      " 1.09291905e-04 2.09282371e-04 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 6.41025641e-03 9.99855861e-01\n",
      " 2.09321162e-04 1.02949899e-04 2.05534355e-04 1.65917305e-04\n",
      " 4.45571899e-01 9.46044922e-04 1.40477718e-04 8.60986547e-01\n",
      " 6.58823529e-01 3.92156863e-02 6.27450980e-02 7.80392157e-01\n",
      " 9.56862745e-01 1.88235294e-01 2.15686275e-01 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(flows_scaled[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T08:42:46.933024Z",
     "start_time": "2018-07-13T08:42:46.925018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the final netflow dataset: (2830743, 47)\n",
      "Outer type: <class 'numpy.ndarray'>\n",
      "Single entry type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the final netflow dataset:\", flows_scaled.shape)\n",
    "print(\"Outer type:\", type(flows_scaled))\n",
    "print(\"Single entry type:\", type(flows_scaled[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, the [Keras Embedding Layer](https://keras.io/layers/embeddings/#embedding) expects a maximum vocabulary size, which we can simply calculate by finding max() in our scaled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T08:42:46.985882Z",
     "start_time": "2018-07-13T08:42:46.933994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum vocabulary size: 2\n"
     ]
    }
   ],
   "source": [
    "#find the maximum vocabulary size\n",
    "voc_size = (flows_scaled.max()+1).astype('int64')\n",
    "print(\"Maximum vocabulary size:\", voc_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use some nice callbacks for the model at hand. As training of LSTM nets is computationally expensive, we'll save the best model to disk.  \n",
    "Furthermore, we'll implement a callback that stops the training process as soon as the accuracy stops impproving.  \n",
    "Finally, we register the tensorboard callback, which allows for detailed insights and nice vizualizations while and after training time.\n",
    "\n",
    "**Also, this is where we define the percentages of train and test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T08:42:48.292853Z",
     "start_time": "2018-07-13T08:42:46.986853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of scaled flows: 2830743\n",
      "No of labels: 2830743\n",
      "Training Set Size: 1981520\n",
      "Validation Set Size: 849223\n"
     ]
    }
   ],
   "source": [
    "# Define some semi-global stuff\n",
    "test_size = 0.3\n",
    "batch_size = 64\n",
    "no_of_classes = len(np.unique(enc_labels))\n",
    "\n",
    "# https://stackoverflow.com/questions/3674409/how-to-split-partition-a-dataset-into-training-and-test-datasets-for-e-g-cros/18544946#18544946\n",
    "# Split training and test data, as the tensorboard embedding stuff needs embedding data, too\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(\"No of scaled flows:\", len(flows_scaled))\n",
    "print(\"No of labels:\", len(enc_labels))\n",
    "\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(flows_scaled, enc_labels, test_size=test_size, shuffle=False)\n",
    "\n",
    "print(\"Training Set Size:\",len(labels_train))\n",
    "print(\"Validation Set Size:\",len(labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T08:42:50.388771Z",
     "start_time": "2018-07-13T08:42:48.293850Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from os.path import exists, join\n",
    "run_date = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "# https://github.com/keras-team/keras/blob/master/examples/tensorboard_embeddings_mnist.py\n",
    "\n",
    "# save the class labels to disk to color data points in TensorBoard accordingly\n",
    "filename = os.path.join('logs','lstm-{}'.format(run_date),'metadata.tsv')\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'w') as f:\n",
    "    np.savetxt(f, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T08:42:52.817967Z",
     "start_time": "2018-07-13T08:42:50.389658Z"
    }
   },
   "outputs": [],
   "source": [
    "# Time for some nice vizualization stuff. Set this up and include as callback, then:\n",
    "# tensorboard --logdir=path/to/logdir\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint,TensorBoard\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='acc', # Which metric to monitor\n",
    "        patience=1     # Interrupt training after acc has stopped improving for more than 1 epoch\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath='models/lstm-{}.h5'.format(run_date),\n",
    "        monitor='val_loss',   \n",
    "        save_best_only=True    # Only save one. Only overwrite this one if val_loss has improved\n",
    "    ),\n",
    "    TensorBoard(\n",
    "        log_dir='logs/lstm-{}'.format(run_date),\n",
    "        #histogram_freq=1,     # Record activation histograms every epoch\n",
    "        #embeddings_freq=1,     # Record embedding data every epoch -> There's something wrong with the embeddings here. Keras crashed with them enabled\n",
    "        #embeddings_layer_names=['LSTMnet'],\n",
    "        #embeddings_metadata='metadata.tsv',\n",
    "        #embeddings_data=data_test,\n",
    "       # batch_size=batch_size\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T08:42:55.134618Z",
     "start_time": "2018-07-13T08:42:52.818964Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1981520 samples, validate on 849223 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-5ca8dde60890>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m                    )\n",
      "\u001b[1;32mc:\\program files\\python3x64\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\program files\\python3x64\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python3x64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python3x64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2628\u001b[0m                                 \u001b[0mfeed_symbols\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2630\u001b[1;33m                                 session)\n\u001b[0m\u001b[0;32m   2631\u001b[0m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python3x64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[1;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[0;32m   2580\u001b[0m         \u001b[0mcallable_opts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2581\u001b[0m         \u001b[1;31m# Create callable.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2582\u001b[1;33m         \u001b[0mcallable_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2583\u001b[0m         \u001b[1;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2584\u001b[0m         \u001b[1;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python3x64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[1;34m(self, callable_options)\u001b[0m\n\u001b[0;32m   1478\u001b[0m     \"\"\"\n\u001b[0;32m   1479\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1480\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python3x64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, session, callable_options)\u001b[0m\n\u001b[0;32m   1436\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1437\u001b[0m             self._handle = tf_session.TF_SessionMakeCallable(\n\u001b[1;32m-> 1438\u001b[1;33m                 session._session, options_ptr, status)\n\u001b[0m\u001b[0;32m   1439\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1440\u001b[0m             self._handle = tf_session.TF_DeprecatedSessionMakeCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# see https://stackoverflow.com/a/49436133/3864726\n",
    "# This is especially important in an environment like Jupyter, where the Kernel keeps on running\n",
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(voc_size, 32)) \n",
    "model.add(LSTM(32, name='LSTMnet'))\n",
    "model.add(Dense(no_of_classes, activation='softmax')) # Multiclass classification. For binary, one would use i.e. sigmoid\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='sparse_categorical_crossentropy', # Multiclass classification! Binary would be binary_crossentropy\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(data_train, labels_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=batch_size,\n",
    "                    verbose=1,\n",
    "                    validation_data=(data_test, labels_test),\n",
    "                    callbacks=callbacks\n",
    "                   )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
