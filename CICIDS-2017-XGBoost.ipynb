{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost with CICIDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides comparsion stats.  \n",
    "The CICIDS2017 version used is [from the University of New Brunswick, Canada](http://www.unb.ca/cic/datasets/ids-2017.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem / ToDo summary:\n",
    "\n",
    "- is my keras layer architecture right? 4 vs. 5 layers\n",
    "    - especially the last layer - image shows 2 nodes -> I am using 1 node with binary crossentropy\n",
    "- Float issue / overflow with infinite values being recognized by numpy/pandas\n",
    "- count & srv_count don't have an appropriate representation in CICIDS\n",
    "- training dropout not defined by authors\n",
    "- activation functions not defined by authors\n",
    "- normalization strategy not defined by authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "flows = pd.DataFrame()\n",
    "\n",
    "datafile_names_sorted = [\n",
    "    'Monday-WorkingHours.pcap_ISCX_clean.csv',\n",
    "    'Tuesday-WorkingHours.pcap_ISCX_clean.csv',\n",
    "    'Wednesday-WorkingHours.pcap_ISCX_clean.csv',\n",
    "    'Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX_clean.csv',\n",
    "    'Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX_clean.csv',\n",
    "    'Friday-WorkingHours-Morning.pcap_ISCX_clean.csv',\n",
    "    'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX_clean.csv',\n",
    "    'Friday-WorkingHours-Afternoon-DDos.pcap_ISCX_clean.csv'\n",
    "]\n",
    "\n",
    "for filename in datafile_names_sorted:\n",
    "    inputFileName = os.path.join('CICIDS2017', filename)\n",
    "    print('Appending', inputFileName)\n",
    "    new_flows = pd.read_csv(inputFileName)\n",
    "    if 'external_ip' not in new_flows: # This field is not in all datafiles\n",
    "            new_flows['external_ip'] = \"0.0.0.0\"\n",
    "    flows = flows.append(new_flows,ignore_index=True,sort=False)\n",
    "\n",
    "print('Found these class labels:', str(flows.label.unique()))\n",
    "flows.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need 6 features, so we create a new DF that only holds them.  \n",
    "The mapping is as follows:  \n",
    "\n",
    "| NSL-KDD field | CICIDS2017 field |\n",
    "|---------------|---------------------|\n",
    "| duration | flow_duration |\n",
    "| protocol_type | protocol |\n",
    "| src_bytes | total_fwd_packets |\n",
    "| dst_bytes | total_backward_packets |\n",
    "| count | flow_packets_per_s |\n",
    "| srv_count | destination_port |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = flows.filter(['flow_duration', 'protocol', 'total_fwd_packets', 'total_backward_packets','flow_packets_per_s','destination_port'], axis=1) \n",
    "#convert into numpy array, as keras seems to like that better\n",
    "features_nd = features.astype('float64').values\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_nd[features_nd == np.inf] = 0 # FIXME: replace with something sensible\n",
    "print(\"Data has NaN:\",np.any(np.isnan(features_nd)))\n",
    "print(\"Data has only finite values\",np.all(np.isfinite(features_nd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# tokenize the LABELS\n",
    "label_tokenizer = Tokenizer(num_words=20, filters='') # don't filter any of the characters. 1 entry = 1 label \n",
    "label_tokenizer.fit_on_texts(flows['label'])\n",
    "\n",
    "# Run the fitted tokenizer on the label column and save the encoded data as dataframe\n",
    "enc_labels = label_tokenizer.texts_to_sequences(flows['label'])\n",
    "enc_labels = np.concatenate(enc_labels).ravel()\n",
    "\n",
    "# as the Encoder documentation states, 0 will never assigned to a label.\n",
    "# I, on the other hand, need an index starting with 0. So we substract 1 of all classes.\n",
    "enc_labels = enc_labels -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 1 if x > 0 else 0\n",
    "f = np.vectorize(f)\n",
    "\n",
    "# We only want to know if it's benign or not, so we switch to 0 or 1\n",
    "enc_labels = f(enc_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.3\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(\"No of flows:\", len(features_nd))\n",
    "print(\"No of labels:\", len(enc_labels))\n",
    "\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(features_nd, enc_labels, test_size=test_size, shuffle=False)\n",
    "\n",
    "print(\"Training Set Size:\",len(labels_train))\n",
    "print(\"Validation Set Size:\",len(labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "#flows_scaled = min_max_scaler.fit_transform(features_nd)\n",
    "data_train = min_max_scaler.fit_transform(data_train)\n",
    "data_test = min_max_scaler.transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the final netflow dataset:\", data_train.shape)\n",
    "print(\"Outer type:\", type(data_train))\n",
    "print(\"Single entry type:\", type(data_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime preqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from os.path import exists, join\n",
    "\n",
    "# Define some semi-global stuff\n",
    "\n",
    "batch_size = 10\n",
    "epochs = 100\n",
    "learn_rate = 0.001\n",
    "\n",
    "run_date = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "runtype_name = 'cicids2017-sdn-dnn'\n",
    "log_folder_path = os.path.join('logs',runtype_name + '-{}'.format(run_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/keras-team/keras/blob/master/examples/tensorboard_embeddings_mnist.py\n",
    "\n",
    "# save the class labels to disk to color data points in TensorBoard accordingly\n",
    "filename = os.path.join(log_folder_path,'metadata.tsv')\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'w') as f:\n",
    "    np.savetxt(f, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    n_jobs=-1,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "xgb.fit(data_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "kdd_test_predicitions = xgb.predict(kdd_test)\n",
    "predicted = [round(value) for value in kdd_test_predicitions]\n",
    "\n",
    "accuracy = accuracy_score(kdd_test_labels, predicted)\n",
    "print(f'Mean accuracy score: {accuracy:.3}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
