{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Approach for Network Intrusion Detection in Software Defined Networking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a practical implementation and adaptation of the paper of Tuan A Tang et al.: [10.1109/WINCOM.2016.7777224](https://doi.org/10.1109/WINCOM.2016.7777224).  \n",
    "Tang et al. built a deep neural network around software defined infrastructure with the target of anomaly-based intrusion detection and archived impressive results.  \n",
    "Besides the practical implementation they made use of the NSL-KDD Dataset.  \n",
    "As I am using the CICIDS2017 dataset, some tuning of input parameters is required. Mostly, the *count* and *srv_count* variables need to be adapted.  \n",
    "These variables, which serve as two of six inputs of the neural network at hand, are calculated as the number of connections to the same host/service as the current connection __in the last two seconds__.  \n",
    "As the CICIDS2017 dataset does not count the number of connections, it stands to be defined how to deal with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem / ToDo summary:\n",
    "\n",
    "- is my keras layer architecture right? 4 vs. 5 layers\n",
    "    - especially the last layer - image shows 2 nodes -> I am using 1 node with binary crossentropy\n",
    "- Float issue / overflow with infinite values being recognized by numpy/pandas\n",
    "- count & srv_count don't have an appropriate representation in CICIDS\n",
    "- training dropout not defined by authors\n",
    "- activation functions not defined by authors\n",
    "- normalization strategy not defined by authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(filename):\n",
    "    filepath = os.path.join('CICIDS2017', filename+'.pkl')\n",
    "    return pd.read_pickle(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cic_train_data = load_df('cic_train_data')\n",
    "cic_test_data = load_df('cic_test_data')\n",
    "cic_train_labels = load_df('cic_train_labels')\n",
    "cic_test_labels = load_df('cic_test_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need 6 features, so we create a new DF that only holds them.  \n",
    "The mapping is as follows:  \n",
    "\n",
    "| NSL-KDD field | CICIDS2017 field |\n",
    "|---------------|---------------------|\n",
    "| duration | flow_duration |\n",
    "| protocol_type | protocol |\n",
    "| src_bytes | total_fwd_packets |\n",
    "| dst_bytes | total_backward_packets |\n",
    "| count | flow_packets_per_s |\n",
    "| srv_count | destination_port |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T15:07:12.318635Z",
     "start_time": "2018-07-27T15:07:09.978564Z"
    }
   },
   "outputs": [],
   "source": [
    "fields = ['flow_duration', 'protocol', 'total_fwd_packets', 'total_backward_packets','flow_packets_per_s','destination_port']\n",
    "\n",
    "cic_train_data = cic_train_data.filter(fields, axis=1) \n",
    "cic_test_data = cic_test_data.filter(fields, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T15:07:12.394719Z",
     "start_time": "2018-07-27T15:07:12.319633Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flow_duration</th>\n",
       "      <th>protocol</th>\n",
       "      <th>total_fwd_packets</th>\n",
       "      <th>total_backward_packets</th>\n",
       "      <th>flow_packets_per_s</th>\n",
       "      <th>destination_port</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.666667e-08</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.166667e-08</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.750561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.166667e-08</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.750561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.166667e-08</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.750561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.833333e-08</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.755108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flow_duration  protocol  total_fwd_packets  total_backward_packets  \\\n",
       "0   6.666667e-08  0.352941           0.000005                     0.0   \n",
       "1   4.166667e-08  0.352941           0.000005                     0.0   \n",
       "2   4.166667e-08  0.352941           0.000005                     0.0   \n",
       "3   4.166667e-08  0.352941           0.000005                     0.0   \n",
       "4   5.833333e-08  0.352941           0.000005                     0.0   \n",
       "\n",
       "   flow_packets_per_s  destination_port  \n",
       "0            0.500000          0.750561  \n",
       "1            0.800000          0.750561  \n",
       "2            0.800000          0.750561  \n",
       "3            0.800000          0.750561  \n",
       "4            0.533333          0.755108  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cic_train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'normal': 1, 'neptune': 2, 'warezclient': 3, 'ipsweep': 4, 'portsweep': 5, 'teardrop': 6, 'nmap': 7, 'satan': 8, 'smurf': 9, 'pod': 10, 'back': 11, 'guess_passwd': 12, 'ftp_write': 13, 'multihop': 14, 'rootkit': 15, 'buffer_overflow': 16, 'imap': 17, 'warezmaster': 18, 'phf': 19, 'land': 20, 'loadmodule': 21, 'spy': 22, 'perl': 23, 'saint': 24, 'mscan': 25, 'apache2': 26, 'snmpgetattack': 27, 'processtable': 28, 'httptunnel': 29, 'ps': 30, 'snmpguess': 31, 'mailbomb': 32, 'named': 33, 'sendmail': 34, 'xterm': 35, 'worm': 36, 'xlock': 37, 'xsnoop': 38, 'sqlattack': 39, 'udpstorm': 40}\n"
     ]
    }
   ],
   "source": [
    "# Load the translation data from the Keras Tokenizer\n",
    "with open(os.path.join('NSL_KDD','kdd_label_wordindex.json')) as json_in:\n",
    "    data = json.load(json_in)\n",
    "    print(data)\n",
    "    normal_index = data['normal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 0 if x == normal_index else 1\n",
    "f = np.vectorize(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>label_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BENIGN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BENIGN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BENIGN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BENIGN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BENIGN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label  label_encoded\n",
       "0  BENIGN              1\n",
       "1  BENIGN              1\n",
       "2  BENIGN              1\n",
       "3  BENIGN              1\n",
       "4  BENIGN              1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cic_train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cic_train_labels = f(cic_train_labels['label_encoded'].values)\n",
    "cic_test_labels = f(cic_test_labels['label_encoded'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size:\t 1839982\n",
      "Training Label Size:\t 1839982\n",
      "Test Set Size:\t\t 990761\n",
      "Test Label Size:\t 990761\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set Size:\\t\",len(cic_train_data))\n",
    "print(\"Training Label Size:\\t\",len(cic_train_labels))\n",
    "print(\"Test Set Size:\\t\\t\",len(cic_test_data))\n",
    "print(\"Test Label Size:\\t\",len(cic_test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime preqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some semi-global stuff\n",
    "\n",
    "batch_size = 10\n",
    "epochs     = 100\n",
    "learn_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T15:07:30.219511Z",
     "start_time": "2018-07-27T15:07:30.209538Z"
    }
   },
   "outputs": [],
   "source": [
    "run_date = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "runtype_name = 'cicids2017-sdn-dnn'\n",
    "log_folder_path = os.path.join('logs',runtype_name + '-{}'.format(run_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T15:07:32.435642Z",
     "start_time": "2018-07-27T15:07:30.220509Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/keras-team/keras/blob/master/examples/tensorboard_embeddings_mnist.py\n",
    "\n",
    "# save the class labels to disk to color data points in TensorBoard accordingly\n",
    "filename = os.path.join(log_folder_path,'metadata.tsv')\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'w') as f:\n",
    "    np.savetxt(f, cic_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T15:07:34.958149Z",
     "start_time": "2018-07-27T15:07:32.436529Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Time for some nice vizualization stuff. Set this up and include as callback, then:\n",
    "# tensorboard --logdir=path/to/logdir\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint,TensorBoard\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        filepath='models/'+runtype_name+'-{}.h5'.format(run_date),\n",
    "        monitor='val_loss',   \n",
    "        save_best_only=True    # Only save one. Only overwrite this one if val_loss has improved\n",
    "    ),\n",
    "    TensorBoard(\n",
    "        log_dir=log_folder_path,\n",
    "        #histogram_freq=1,     # Record activation histograms every epoch\n",
    "        #embeddings_freq=1,     # Record embedding data every epoch -> There's something wrong with the embeddings here. Keras crashed with them enabled\n",
    "        #embeddings_layer_names=['LSTMnet'],\n",
    "        #embeddings_metadata='metadata.tsv',\n",
    "        #embeddings_data=data_test,\n",
    "       # batch_size=batch_size\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T15:07:46.562273Z",
     "start_time": "2018-07-27T15:07:34.959146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                84        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 78        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 21        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 8         \n",
      "=================================================================\n",
      "Total params: 191\n",
      "Trainable params: 191\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1839982 samples, validate on 990761 samples\n",
      "Epoch 1/100\n",
      "  95210/1839982 [>.............................] - ETA: 2:35 - loss: 0.1915 - acc: 0.9253"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# see implementation/sdn-dnn.py for details, alternatives and comments\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(12, activation='relu', input_dim=6))\n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(RMSprop(lr=learn_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(cic_train_data, cic_train_labels, \n",
    "                    epochs=100, \n",
    "                    batch_size=batch_size,\n",
    "                    verbose=1,\n",
    "                    validation_data=(cic_test_data, cic_test_labels),\n",
    "                    callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
