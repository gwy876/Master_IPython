{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle CICIDS 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks intended use is to load the CSV data into a Pandas dataframe, normalize and scale the data, then write the DataFrame into a pickle to save these steps for every ML framework run.  \n",
    "The output are two pickle files: cic_test_data and cic_test_labels.  \n",
    "**TODO**: Split these into training and test in a meaningful way! (Most likely by hand?)\n",
    "These pickles can be restored as dataframes by calling [pandas.read_pickle()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_pickle.html).  \n",
    "**Hint**: If your are missing the *_clean* CSVs, try running the notebook *Data Sanitazation.ipynb*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import preprocessing\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there is literally \"Inifnity\" written in the CSV dataset, we set an additional filter so that these will be replaced by a NaN-representation (that will lateron be set to zero).  \n",
    "Also, the external_ip field is set to 0.0.0.0 if either NaN or non existent.  \n",
    "Finally, the dtype for the external_ip column had to be set manually to object as Pandas kept getting confused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cic_data = pd.DataFrame()\n",
    "\n",
    "datafile_names_sorted = [\n",
    "    'Monday-WorkingHours.pcap_ISCX_clean.csv',\n",
    "    'Tuesday-WorkingHours.pcap_ISCX_clean.csv',\n",
    "    'Wednesday-WorkingHours.pcap_ISCX_clean.csv',\n",
    "    'Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX_clean.csv',\n",
    "    'Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX_clean.csv',\n",
    "    'Friday-WorkingHours-Morning.pcap_ISCX_clean.csv',\n",
    "    'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX_clean.csv',\n",
    "    'Friday-WorkingHours-Afternoon-DDos.pcap_ISCX_clean.csv'\n",
    "]\n",
    "\n",
    "for filename in datafile_names_sorted:\n",
    "    inputFileName = os.path.join('CICIDS2017', filename)\n",
    "    print('Appending', inputFileName)\n",
    "    \n",
    "    new_flows = pd.read_csv(\n",
    "        inputFileName,\n",
    "        na_values=\"Infinity\",\n",
    "        dtype={'external_ip':'object'},\n",
    "        parse_dates=['timestamp']\n",
    "    )\n",
    "    \n",
    "    # as this field is not in all flows, double check for it\n",
    "    if 'external_ip' not in new_flows:\n",
    "            new_flows['external_ip'] = \"0.0.0.0\"\n",
    "    new_flows['external_ip'].fillna(\"0.0.0.0\", inplace=True)\n",
    "    \n",
    "    cic_data = cic_data.append(new_flows,ignore_index=True,sort=False)\n",
    "\n",
    "print('Found these class labels:', str(cic_data.label.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cic_data.fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cic_data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's still a problem: How can we encode IP addresses in a way that the neural network can make use of them while preserving the hierarchical information they contain?  \n",
    "Encoding IPs through One Hot lets comlexity and training times explode, so for now I am splitting each IP into its four octet pairs and interpret them as numbers.  \n",
    "Maybe there's a better way to represent them (especially because I am only able to encode IPv4 right now).  \n",
    "\n",
    "**Important**: If this breaks, you forgot to remove the broken external IP in Friday DDoS @ 2017-07-07T15:58:00,26794"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/14745022/how-to-split-a-column-into-two-columns\n",
    "# FIXME: Right now, only IPv4 (4 octets)\n",
    "\n",
    "# Split the String representation of the IP into it's four octects, which are delimited by a dot\n",
    "cic_data['source_ip_o1'],cic_data['source_ip_o2'],cic_data['source_ip_o3'],cic_data['source_ip_o4'] = cic_data['source_ip'].str.split('.').str\n",
    "cic_data['destination_ip_o1'],cic_data['destination_ip_o2'],cic_data['destination_ip_o3'],cic_data['destination_ip_o4'] = cic_data['destination_ip'].str.split('.').str\n",
    "cic_data['external_ip_o1'],cic_data['external_ip_o2'],cic_data['external_ip_o3'],cic_data['external_ip_o4'] = cic_data['external_ip'].str.split('.').str\n",
    "\n",
    "# After completion, drop the initial columns, as they aren't needed anymore\n",
    "cic_data.drop(['source_ip'], axis=1, inplace=True)\n",
    "cic_data.drop(['destination_ip'], axis=1, inplace=True)\n",
    "cic_data.drop(['external_ip'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we're going to normalize the dataset, we should drop the flow_id, as this info cannot be normalized.\n",
    "cic_data.drop(['flow_id'], axis=1, inplace=True)\n",
    "\n",
    "# Finally, let's inspect the outcome\n",
    "cic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that this is out of the way, we still need to encode the labels column to numeric values.  \n",
    "To do this, I'm going to be using the Keras Tokenizer.  \n",
    "The labels of the dataset (as in: *Benign*, *DDoS*, *Portscan*, etc) are converted into a list of integers and split off of the main DataFrame.  \n",
    "After this step there is a variable `cic_labels` that holds an integer-encoded list of labels.  \n",
    "A humble example (not representative):  \n",
    "\n",
    "|Label         | Value          |\n",
    "|------------- |---------:|\n",
    "|Benign      | 0|\n",
    "|DDoS        | 1|\n",
    "|Portscan    | 2|  \n",
    "\n",
    "So if the order of the first three Netflows would be *Benign*, *Benign*, *DDos*,  \n",
    "the resulting `enc_labels` would look like this: `[1,1,2]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cic_labels = cic_data.filter(['label'])\n",
    "cic_data.drop(['label'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_classes = len(cic_labels['label'].unique())\n",
    "\n",
    "# FIXME: don't fit the tokenizer on the full ds! Split off a test set first!\n",
    "\n",
    "# tokenize the LABELS\n",
    "label_tokenizer = Tokenizer(num_words=number_of_classes+1, filters='') # don't filter any of the characters. 1 entry = 1 label \n",
    "label_tokenizer.fit_on_texts(cic_labels['label'].unique())\n",
    "\n",
    "# Run the fitted tokenizer on the label column and save the encoded data as dataframe\n",
    "enc_labels = label_tokenizer.texts_to_sequences(cic_labels['label'])\n",
    "\n",
    "# finally, append the encoded labels to the label dataframe\n",
    "cic_labels = pd.concat([cic_labels, pd.DataFrame(columns=['label_encoded'],dtype=np.int8,data=enc_labels)], axis=1)\n",
    "cic_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to translate the encoded labels back, write the Tokenizer wordlist to a file near the CSVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join('CICIDS2017','cic_label_wordindex.json')\n",
    "print('Writing encoder data to file {}: {}'.format(filename, label_tokenizer.word_index))\n",
    "with open(filename, 'w') as outfile:\n",
    "    json.dump(label_tokenizer.word_index, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, no non-number-stuff should remain in cic_data.  \n",
    "Let's check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the scaler converts the DataFrames to numpy arrays, save the header info to recreate a DataFrame afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cic_data_header = list(cic_data.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As many ML implementations behave badly if confronted with non-scaled inputs, we go ahead and transform all features to center, then scale it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaler.fit(cic_data) # fit the scaler on the training data\n",
    "\n",
    "# transform samples without any refitting\n",
    "cic_data = scaler.transform(cic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the DataFrame\n",
    "cic_data = pd.DataFrame(columns=cic_data_header, data=cic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So at this point, we have training and test sets with data and labels. The data parts are encoded and scaled, the encoded indizes are written away as json files.  \n",
    "It would be nice if this data could be used for future runs, right? Right!  \n",
    "That's why we serialize each dataframe into a python binary pickle on it's own (which is a feature directly supported by [Pandas](https://pandas.pydata.org/pandas-docs/stable/api.html#id12) - nice, eh?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_pickle(dataframe, filename):\n",
    "    dataframe.to_pickle(os.path.join('CICIDS2017', filename+'.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_pickle(cic_data, 'cic_data')\n",
    "write_to_pickle(cic_labels, 'cic_labels')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
