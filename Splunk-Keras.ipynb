{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tying it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Fran√ßois Chollet recommends in his excellent book, [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python), it is always a good idea to follow a general workflow.  \n",
    "This is why I adapt the workflow that is introduced in chapter 6.3.  \n",
    "Additionally, I am querying Splunk instead of using a static dataset that would be provided normally.  \n",
    "\n",
    "**On a side note**: I decided to pass on usung a generator for querying the static CICIDS2017 dataset, because Keras does not allow advanced collection of statistics for Tensorboard with generators.  \n",
    "Generators will be most likely neccessary for live analysis of data - a scenario, where advanced insights are not of critical importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Data Out Of Splunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at the IPython notebook \"Splunk Python API - CICIDS17\" for a detailed Splunk howto\n",
    "import splunklib.client as client\n",
    "import splunklib.results as results\n",
    "\n",
    "%run -i splunk_credentials\n",
    "\n",
    "service = client.connect(\n",
    "    host=HOST,\n",
    "    port=PORT,\n",
    "    username=USERNAME,\n",
    "    password=PASSWORD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with setting up the search params as well as the search itself.  \n",
    "For this first example, I am just interested in the part set *Friday Working Hours - Afternoon DDoS* to keep initial training durations low.  \n",
    "Have a look at the IPython notebook *Data Sanitization* for more information on how I transformed the dataset into its current representation.\n",
    "\n",
    "The data has been imported in the index *cicids17*, the CSV data is split by days and attacks.  \n",
    "This is why I can simply query for the source file name that yields the specific DDoS dataset part. \n",
    "In a live data scenario one would narrow down the search time range to drastically speed up search times, but as this is static data, I'm willfully ignoring this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs_oneshot = {'earliest_time': '2017-07-07T16:10:01.000', # Subset start just befor the ddos\n",
    "                  'latest_time':   '2017-07-07T23:59:59.000',\n",
    "                  'count': 0}\n",
    "\n",
    "# This yields the full dataset in its (mostly) raw form\n",
    "#searchquery_oneshot = 'search index=cicids17 source=\"Friday-WorkingHours-Afternoon-DDos.pcap_ISCX_clean.csv\" | sort _time | head 20'\n",
    "\n",
    "# An example of how to prefilter and work with data on the server side\n",
    "searchquery_oneshot = 'search index=cicids17 source=\"Friday-WorkingHours-Afternoon-DDos.pcap_ISCX_clean.csv\" |  table * |  sort 0 timestamp | head 10000'\n",
    "\n",
    "oneshotsearch_results = service.jobs.oneshot(searchquery_oneshot, **kwargs_oneshot) \n",
    "reader = results.ResultsReader(oneshotsearch_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, iterate over all returned entries and convert them into a useable, in-memory datastructure.  \n",
    "Splunk returns an [ordered dict](https://docs.python.org/3/library/collections.html#collections.OrderedDict) as datastructure, which is very convenient for further usage.  \n",
    "Unfortunately, the dict also contains meta info from Splunk, so the entries has to be scrubbed from these.  \n",
    "Additionally, the flow_id and timestamp fields are removed, as they change constantly and could possibly throw off the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "data = []\n",
    "\n",
    "for idx,item in enumerate(reader):  \n",
    "    # delete all splunk-specific metadata as well as the flow_id and timestamp of the original data\n",
    "    try:\n",
    "        del item['flow_id'],item['timestamp']\n",
    "        del item['date_mday'],item['source'],item['index'],item['sourcetype'],item['_subsecond'],item['linecount'],item['_bkt'],item['_raw'],item['date_month'],item['date_year']\n",
    "        del item['_time'],item['timeendpos'],item['timestartpos'],item['date_hour'],item['date_minute'],item['_cd'],item['date_zone'],item['host'],item['_serial']\n",
    "        del item['date_second'],item['_sourcetype'],item['date_wday'],item['splunk_server'],item['punct'],item['_indextime'],item['_si']\n",
    "    except KeyError as ke:\n",
    "        pass # FIXME: Just ignore any key errors for now.\n",
    "    data.append(item)\n",
    "    \n",
    "netflows = pd.DataFrame(data)\n",
    "print('Processed {} netflows'.format(len(netflows)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflows.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splunk returns the newest entries at the top, which normally is a great idea. We, however need the correct spatial alignment.  \n",
    "The Splunk Search API has a *sort* feature; however, it seems to act up sometimes on returning the full dataset. Clear **FIXME**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has a nice built in function for One Hot Encoding: [get_dummies](http://queirozf.com/entries/one-hot-encoding-a-feature-on-a-pandas-dataframe-an-example).  \n",
    "The generated labels are not added to the dataset, as these are used as target values in a standalone DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "df_label = to_categorical(netflows['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate OHE labels\n",
    "df_source_ip = pd.get_dummies(netflows['source_ip'], prefix='source_ip')\n",
    "df_destination_ip = pd.get_dummies(netflows['destination_ip'], prefix='destination_ip')\n",
    "\n",
    "#Append the label columns to the dataset\n",
    "netflows = pd.concat([netflows, df_source_ip], axis=1)\n",
    "netflows = pd.concat([netflows, df_destination_ip], axis=1)\n",
    "\n",
    "#drop the original columns\n",
    "netflows.drop(['source_ip'], axis=1, inplace=True)\n",
    "netflows.drop(['destination_ip'], axis=1, inplace=True)\n",
    "netflows.drop(['label'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflows.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the tensorboard callbacks and logdir stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from keras.callbacks import TensorBoard\n",
    "tensorboard = TensorBoard(log_dir=\"logs/rms-{}\".format(time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into test and validation parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = netflows.values[:5000]\n",
    "x_part_train = netflows.values[5000:]\n",
    "\n",
    "y_val = df_label.values[:5000]\n",
    "y_part_train = df_label.values[5000:]\n",
    "\n",
    "print(x_part_train.shape)\n",
    "\n",
    "import numpy as np\n",
    "# Reshape all imputs to a 3D-tensor: https://stackoverflow.com/questions/44704435/error-when-checking-model-input-expected-lstm-1-input-to-have-3-dimensions-but\n",
    "#x_val = np.reshape(x_val, (x_val.shape[0], 1, x_val.shape[1]))\n",
    "#x_part_train = np.reshape(x_part_train, (x_part_train.shape[0], 1, x_part_train.shape[1]))\n",
    "#y_val = np.reshape(y_val, (y_val.shape[0], 1, y_val.shape[1]))\n",
    "#y_part_train = np.reshape(y_part_train, (y_part_train.shape[0], 1, y_part_train.shape[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first simple RMSProp approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(x_part_train.shape[-1], output_dim=256))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_part_train, y_part_train, batch_size=16, epochs=2)\n",
    "score = model.evaluate(x_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
