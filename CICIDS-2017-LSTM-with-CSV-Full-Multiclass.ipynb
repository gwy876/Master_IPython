{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T09:06:00.659172Z",
     "start_time": "2018-07-13T09:05:09.198451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending CICIDS2017/Monday-WorkingHours.pcap_ISCX_clean.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matzilla/Entwicklung/Master_IPython/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending CICIDS2017/Tuesday-WorkingHours.pcap_ISCX_clean.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matzilla/Entwicklung/Master_IPython/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (20,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending CICIDS2017/Wednesday-WorkingHours.pcap_ISCX_clean.csv\n",
      "Appending CICIDS2017/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX_clean.csv\n",
      "Appending CICIDS2017/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX_clean.csv\n",
      "Appending CICIDS2017/Friday-WorkingHours-Morning.pcap_ISCX_clean.csv\n",
      "Appending CICIDS2017/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX_clean.csv\n",
      "Appending CICIDS2017/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX_clean.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matzilla/Entwicklung/Master_IPython/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (20,21,85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found these class labels: ['BENIGN' 'FTPPatator' 'SSHPatator' 'DoSSlowloris' 'DoSSlowhttptest'\n",
      " 'DoSHulk' 'DoSGoldenEye' 'Heartbleed' 'BruteForce' 'XSS' 'SQLInjection'\n",
      " 'Infiltration' 'Bot' 'PortScan' 'DDoS']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "flows = pd.DataFrame()\n",
    "\n",
    "datafile_names_sorted = [\n",
    "    'Monday-WorkingHours.pcap_ISCX_clean.csv',\n",
    "    'Tuesday-WorkingHours.pcap_ISCX_clean.csv',\n",
    "    'Wednesday-WorkingHours.pcap_ISCX_clean.csv',\n",
    "    'Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX_clean.csv',\n",
    "    'Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX_clean.csv',\n",
    "    'Friday-WorkingHours-Morning.pcap_ISCX_clean.csv',\n",
    "    'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX_clean.csv',\n",
    "    'Friday-WorkingHours-Afternoon-DDos.pcap_ISCX_clean.csv'\n",
    "]\n",
    "\n",
    "for filename in datafile_names_sorted:\n",
    "    inputFileName = os.path.join('CICIDS2017', filename)\n",
    "    print('Appending', inputFileName)\n",
    "    new_flows = pd.read_csv(inputFileName)\n",
    "    \n",
    "    #as this field is not in all flows, double check for it\n",
    "    if 'external_ip' not in new_flows:\n",
    "            new_flows['external_ip'] = \"0.0.0.0\"\n",
    "    new_flows['external_ip'].fillna(\"0.0.0.0\", inplace=True)\n",
    "    \n",
    "    flows = flows.append(new_flows,ignore_index=True,sort=False)\n",
    "\n",
    "print('Found these class labels:', str(flows.label.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of additional, calculated field in the CIC dataset. Whilst these are interesting to have for research purposes, I am mostly interested to stay as close to conventional netflows.  \n",
    "For starters, we'll drop the flow_id as well as the timestamp, as both fields introduce problems and are irrelevant.  \n",
    "As a continuation, we'll drop most of the remaining calculated field to keep the [Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) at bay and keep complexity and training times under control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T09:06:03.275101Z",
     "start_time": "2018-07-13T09:06:00.663076Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_ip</th>\n",
       "      <th>source_port</th>\n",
       "      <th>destination_ip</th>\n",
       "      <th>destination_port</th>\n",
       "      <th>protocol</th>\n",
       "      <th>flow_duration</th>\n",
       "      <th>total_fwd_packets</th>\n",
       "      <th>total_backward_packets</th>\n",
       "      <th>total_length_of_fwd_packets</th>\n",
       "      <th>total_length_of_bwd_packets</th>\n",
       "      <th>flow_bytes_per_s</th>\n",
       "      <th>flow_packets_per_s</th>\n",
       "      <th>fwd_psh_flags</th>\n",
       "      <th>bwd_psh_flags</th>\n",
       "      <th>fwd_urg_flags</th>\n",
       "      <th>bwd_urg_flags</th>\n",
       "      <th>fwd_header_length</th>\n",
       "      <th>bwd_header_length</th>\n",
       "      <th>fwd_packets_per_s</th>\n",
       "      <th>bwd_packets_per_s</th>\n",
       "      <th>fin_flag_count</th>\n",
       "      <th>syn_flag_count</th>\n",
       "      <th>rst_flag_count</th>\n",
       "      <th>psh_flag_count</th>\n",
       "      <th>ack_flag_count</th>\n",
       "      <th>urg_flag_count</th>\n",
       "      <th>cwe_flag_count</th>\n",
       "      <th>ece_flag_count</th>\n",
       "      <th>down_per_up_ratio</th>\n",
       "      <th>fwd_header_length.1</th>\n",
       "      <th>subflow_fwd_packets</th>\n",
       "      <th>subflow_fwd_bytes</th>\n",
       "      <th>subflow_bwd_packets</th>\n",
       "      <th>subflow_bwd_bytes</th>\n",
       "      <th>init_win_bytes_forward</th>\n",
       "      <th>init_win_bytes_backward</th>\n",
       "      <th>act_data_pkt_fwd</th>\n",
       "      <th>label</th>\n",
       "      <th>external_ip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.254.250.126</td>\n",
       "      <td>80</td>\n",
       "      <td>192.168.10.5</td>\n",
       "      <td>49188</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3000000.0</td>\n",
       "      <td>500000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>5.000000e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>329</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>0.0.0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.254.250.126</td>\n",
       "      <td>80</td>\n",
       "      <td>192.168.10.5</td>\n",
       "      <td>49188</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2E7</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>329</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>0.0.0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.254.250.126</td>\n",
       "      <td>80</td>\n",
       "      <td>192.168.10.5</td>\n",
       "      <td>49188</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2E7</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>329</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>0.0.0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.254.250.126</td>\n",
       "      <td>80</td>\n",
       "      <td>192.168.10.5</td>\n",
       "      <td>49188</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2E7</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>329</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>0.0.0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.253.185.121</td>\n",
       "      <td>80</td>\n",
       "      <td>192.168.10.14</td>\n",
       "      <td>49486</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4000000.0</td>\n",
       "      <td>666666.6666666666</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>6.666667e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>245</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>0.0.0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       source_ip  source_port destination_ip  destination_port  protocol  \\\n",
       "0  8.254.250.126           80   192.168.10.5             49188         6   \n",
       "1  8.254.250.126           80   192.168.10.5             49188         6   \n",
       "2  8.254.250.126           80   192.168.10.5             49188         6   \n",
       "3  8.254.250.126           80   192.168.10.5             49188         6   \n",
       "4  8.253.185.121           80  192.168.10.14             49486         6   \n",
       "\n",
       "   flow_duration  total_fwd_packets  total_backward_packets  \\\n",
       "0              4                  2                       0   \n",
       "1              1                  2                       0   \n",
       "2              1                  2                       0   \n",
       "3              1                  2                       0   \n",
       "4              3                  2                       0   \n",
       "\n",
       "   total_length_of_fwd_packets  total_length_of_bwd_packets flow_bytes_per_s  \\\n",
       "0                         12.0                          0.0        3000000.0   \n",
       "1                         12.0                          0.0            1.2E7   \n",
       "2                         12.0                          0.0            1.2E7   \n",
       "3                         12.0                          0.0            1.2E7   \n",
       "4                         12.0                          0.0        4000000.0   \n",
       "\n",
       "  flow_packets_per_s  fwd_psh_flags  bwd_psh_flags  fwd_urg_flags  \\\n",
       "0           500000.0              0              0              0   \n",
       "1          2000000.0              0              0              0   \n",
       "2          2000000.0              0              0              0   \n",
       "3          2000000.0              0              0              0   \n",
       "4  666666.6666666666              0              0              0   \n",
       "\n",
       "   bwd_urg_flags  fwd_header_length  bwd_header_length  fwd_packets_per_s  \\\n",
       "0              0                 40                  0       5.000000e+05   \n",
       "1              0                 40                  0       2.000000e+06   \n",
       "2              0                 40                  0       2.000000e+06   \n",
       "3              0                 40                  0       2.000000e+06   \n",
       "4              0                 40                  0       6.666667e+05   \n",
       "\n",
       "   bwd_packets_per_s  fin_flag_count  syn_flag_count  rst_flag_count  \\\n",
       "0                0.0               0               0               0   \n",
       "1                0.0               0               0               0   \n",
       "2                0.0               0               0               0   \n",
       "3                0.0               0               0               0   \n",
       "4                0.0               0               0               0   \n",
       "\n",
       "   psh_flag_count  ack_flag_count  urg_flag_count  cwe_flag_count  \\\n",
       "0               0               1               1               0   \n",
       "1               0               1               1               0   \n",
       "2               0               1               1               0   \n",
       "3               0               1               1               0   \n",
       "4               0               1               1               0   \n",
       "\n",
       "   ece_flag_count  down_per_up_ratio  fwd_header_length.1  \\\n",
       "0               0                0.0                   40   \n",
       "1               0                0.0                   40   \n",
       "2               0                0.0                   40   \n",
       "3               0                0.0                   40   \n",
       "4               0                0.0                   40   \n",
       "\n",
       "   subflow_fwd_packets  subflow_fwd_bytes  subflow_bwd_packets  \\\n",
       "0                    2                 12                    0   \n",
       "1                    2                 12                    0   \n",
       "2                    2                 12                    0   \n",
       "3                    2                 12                    0   \n",
       "4                    2                 12                    0   \n",
       "\n",
       "   subflow_bwd_bytes  init_win_bytes_forward  init_win_bytes_backward  \\\n",
       "0                  0                     329                       -1   \n",
       "1                  0                     329                       -1   \n",
       "2                  0                     329                       -1   \n",
       "3                  0                     329                       -1   \n",
       "4                  0                     245                       -1   \n",
       "\n",
       "   act_data_pkt_fwd   label external_ip  \n",
       "0                 1  BENIGN     0.0.0.0  \n",
       "1                 1  BENIGN     0.0.0.0  \n",
       "2                 1  BENIGN     0.0.0.0  \n",
       "3                 1  BENIGN     0.0.0.0  \n",
       "4                 1  BENIGN     0.0.0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop unused fields.\n",
    "unused_fields = ['flow_id','timestamp','fwd_packet_length_max','fwd_packet_length_min','fwd_packet_length_mean','fwd_packet_length_std','bwd_packet_length_max','bwd_packet_length_min','bwd_packet_length_mean','bwd_packet_length_std','flow_iat_mean','flow_iat_std','flow_iat_max','flow_iat_min','fwd_iat_total','fwd_iat_mean','fwd_iat_std','fwd_iat_max','fwd_iat_min','bwd_iat_total','bwd_iat_mean','bwd_iat_std','bwd_iat_max','bwd_iat_min','min_packet_length','max_packet_length','packet_length_mean','packet_length_std','packet_length_variance','average_packet_size','avg_fwd_segment_size','avg_bwd_segment_size','fwd_avg_bytes_per_bulk','fwd_avg_packets_per_bulk','fwd_avg_bulk_rate','bwd_avg_bytes_per_bulk','bwd_avg_packets_per_bulk','bwd_avg_bulk_rate','active_mean','active_std','active_max','active_min','idle_mean','idle_std','idle_max','idle_min','min_seg_size_forward']\n",
    "flows.drop(unused_fields, axis=1, inplace=True)\n",
    "flows.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's still a problem: How can we encode IP addresses in a way that the neural network can make use of them while preserving the hierarchical information they contain?  \n",
    "Encoding IPs through One Hot let's comlexity and training times explode, so for now I am splitting each IP into its four octet pairs and interpret them as numbers.  \n",
    "Maybe there's a better way to represent them (especially because I am only able to encode IPv4 right now).  \n",
    "\n",
    "**Important**: If this breaks, you forgot to remove the broken external IP in Friday DDoS @ 2017-07-07T15:58:00,26794"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T09:06:29.982498Z",
     "start_time": "2018-07-13T09:06:03.278029Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_port</th>\n",
       "      <th>destination_port</th>\n",
       "      <th>protocol</th>\n",
       "      <th>flow_duration</th>\n",
       "      <th>total_fwd_packets</th>\n",
       "      <th>total_backward_packets</th>\n",
       "      <th>total_length_of_fwd_packets</th>\n",
       "      <th>total_length_of_bwd_packets</th>\n",
       "      <th>flow_bytes_per_s</th>\n",
       "      <th>flow_packets_per_s</th>\n",
       "      <th>fwd_psh_flags</th>\n",
       "      <th>bwd_psh_flags</th>\n",
       "      <th>fwd_urg_flags</th>\n",
       "      <th>bwd_urg_flags</th>\n",
       "      <th>fwd_header_length</th>\n",
       "      <th>bwd_header_length</th>\n",
       "      <th>fwd_packets_per_s</th>\n",
       "      <th>bwd_packets_per_s</th>\n",
       "      <th>fin_flag_count</th>\n",
       "      <th>syn_flag_count</th>\n",
       "      <th>rst_flag_count</th>\n",
       "      <th>psh_flag_count</th>\n",
       "      <th>ack_flag_count</th>\n",
       "      <th>urg_flag_count</th>\n",
       "      <th>cwe_flag_count</th>\n",
       "      <th>ece_flag_count</th>\n",
       "      <th>down_per_up_ratio</th>\n",
       "      <th>fwd_header_length.1</th>\n",
       "      <th>subflow_fwd_packets</th>\n",
       "      <th>subflow_fwd_bytes</th>\n",
       "      <th>subflow_bwd_packets</th>\n",
       "      <th>subflow_bwd_bytes</th>\n",
       "      <th>init_win_bytes_forward</th>\n",
       "      <th>init_win_bytes_backward</th>\n",
       "      <th>act_data_pkt_fwd</th>\n",
       "      <th>label</th>\n",
       "      <th>source_ip_o1</th>\n",
       "      <th>source_ip_o2</th>\n",
       "      <th>source_ip_o3</th>\n",
       "      <th>source_ip_o4</th>\n",
       "      <th>destination_ip_o1</th>\n",
       "      <th>destination_ip_o2</th>\n",
       "      <th>destination_ip_o3</th>\n",
       "      <th>destination_ip_o4</th>\n",
       "      <th>external_ip_o1</th>\n",
       "      <th>external_ip_o2</th>\n",
       "      <th>external_ip_o3</th>\n",
       "      <th>external_ip_o4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80</td>\n",
       "      <td>49188</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3000000.0</td>\n",
       "      <td>500000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>5.000000e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>329</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>8</td>\n",
       "      <td>254</td>\n",
       "      <td>250</td>\n",
       "      <td>126</td>\n",
       "      <td>192</td>\n",
       "      <td>168</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80</td>\n",
       "      <td>49188</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2E7</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>329</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>8</td>\n",
       "      <td>254</td>\n",
       "      <td>250</td>\n",
       "      <td>126</td>\n",
       "      <td>192</td>\n",
       "      <td>168</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80</td>\n",
       "      <td>49188</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2E7</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>329</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>8</td>\n",
       "      <td>254</td>\n",
       "      <td>250</td>\n",
       "      <td>126</td>\n",
       "      <td>192</td>\n",
       "      <td>168</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80</td>\n",
       "      <td>49188</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2E7</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>329</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>8</td>\n",
       "      <td>254</td>\n",
       "      <td>250</td>\n",
       "      <td>126</td>\n",
       "      <td>192</td>\n",
       "      <td>168</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80</td>\n",
       "      <td>49486</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4000000.0</td>\n",
       "      <td>666666.6666666666</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>6.666667e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>245</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>8</td>\n",
       "      <td>253</td>\n",
       "      <td>185</td>\n",
       "      <td>121</td>\n",
       "      <td>192</td>\n",
       "      <td>168</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source_port  destination_port  protocol  flow_duration  total_fwd_packets  \\\n",
       "0           80             49188         6              4                  2   \n",
       "1           80             49188         6              1                  2   \n",
       "2           80             49188         6              1                  2   \n",
       "3           80             49188         6              1                  2   \n",
       "4           80             49486         6              3                  2   \n",
       "\n",
       "   total_backward_packets  total_length_of_fwd_packets  \\\n",
       "0                       0                         12.0   \n",
       "1                       0                         12.0   \n",
       "2                       0                         12.0   \n",
       "3                       0                         12.0   \n",
       "4                       0                         12.0   \n",
       "\n",
       "   total_length_of_bwd_packets flow_bytes_per_s flow_packets_per_s  \\\n",
       "0                          0.0        3000000.0           500000.0   \n",
       "1                          0.0            1.2E7          2000000.0   \n",
       "2                          0.0            1.2E7          2000000.0   \n",
       "3                          0.0            1.2E7          2000000.0   \n",
       "4                          0.0        4000000.0  666666.6666666666   \n",
       "\n",
       "   fwd_psh_flags  bwd_psh_flags  fwd_urg_flags  bwd_urg_flags  \\\n",
       "0              0              0              0              0   \n",
       "1              0              0              0              0   \n",
       "2              0              0              0              0   \n",
       "3              0              0              0              0   \n",
       "4              0              0              0              0   \n",
       "\n",
       "   fwd_header_length  bwd_header_length  fwd_packets_per_s  bwd_packets_per_s  \\\n",
       "0                 40                  0       5.000000e+05                0.0   \n",
       "1                 40                  0       2.000000e+06                0.0   \n",
       "2                 40                  0       2.000000e+06                0.0   \n",
       "3                 40                  0       2.000000e+06                0.0   \n",
       "4                 40                  0       6.666667e+05                0.0   \n",
       "\n",
       "   fin_flag_count  syn_flag_count  rst_flag_count  psh_flag_count  \\\n",
       "0               0               0               0               0   \n",
       "1               0               0               0               0   \n",
       "2               0               0               0               0   \n",
       "3               0               0               0               0   \n",
       "4               0               0               0               0   \n",
       "\n",
       "   ack_flag_count  urg_flag_count  cwe_flag_count  ece_flag_count  \\\n",
       "0               1               1               0               0   \n",
       "1               1               1               0               0   \n",
       "2               1               1               0               0   \n",
       "3               1               1               0               0   \n",
       "4               1               1               0               0   \n",
       "\n",
       "   down_per_up_ratio  fwd_header_length.1  subflow_fwd_packets  \\\n",
       "0                0.0                   40                    2   \n",
       "1                0.0                   40                    2   \n",
       "2                0.0                   40                    2   \n",
       "3                0.0                   40                    2   \n",
       "4                0.0                   40                    2   \n",
       "\n",
       "   subflow_fwd_bytes  subflow_bwd_packets  subflow_bwd_bytes  \\\n",
       "0                 12                    0                  0   \n",
       "1                 12                    0                  0   \n",
       "2                 12                    0                  0   \n",
       "3                 12                    0                  0   \n",
       "4                 12                    0                  0   \n",
       "\n",
       "   init_win_bytes_forward  init_win_bytes_backward  act_data_pkt_fwd   label  \\\n",
       "0                     329                       -1                 1  BENIGN   \n",
       "1                     329                       -1                 1  BENIGN   \n",
       "2                     329                       -1                 1  BENIGN   \n",
       "3                     329                       -1                 1  BENIGN   \n",
       "4                     245                       -1                 1  BENIGN   \n",
       "\n",
       "  source_ip_o1 source_ip_o2 source_ip_o3 source_ip_o4 destination_ip_o1  \\\n",
       "0            8          254          250          126               192   \n",
       "1            8          254          250          126               192   \n",
       "2            8          254          250          126               192   \n",
       "3            8          254          250          126               192   \n",
       "4            8          253          185          121               192   \n",
       "\n",
       "  destination_ip_o2 destination_ip_o3 destination_ip_o4 external_ip_o1  \\\n",
       "0               168                10                 5              0   \n",
       "1               168                10                 5              0   \n",
       "2               168                10                 5              0   \n",
       "3               168                10                 5              0   \n",
       "4               168                10                14              0   \n",
       "\n",
       "  external_ip_o2 external_ip_o3 external_ip_o4  \n",
       "0              0              0              0  \n",
       "1              0              0              0  \n",
       "2              0              0              0  \n",
       "3              0              0              0  \n",
       "4              0              0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/14745022/how-to-split-a-column-into-two-columns\n",
    "# FIXME: Right now, only IPv4 (4 octets)\n",
    "\n",
    "# Split the String representation of the IP into it's four octects, which are delimited by a dot\n",
    "flows['source_ip_o1'],flows['source_ip_o2'],flows['source_ip_o3'],flows['source_ip_o4'] = flows['source_ip'].str.split('.').str\n",
    "flows['destination_ip_o1'],flows['destination_ip_o2'],flows['destination_ip_o3'],flows['destination_ip_o4'] = flows['destination_ip'].str.split('.').str\n",
    "flows['external_ip_o1'],flows['external_ip_o2'],flows['external_ip_o3'],flows['external_ip_o4'] = flows['external_ip'].str.split('.').str\n",
    "\n",
    "# After completion, drop the initial columns, as they aren't needed anymore\n",
    "flows.drop(['source_ip'], axis=1, inplace=True)\n",
    "flows.drop(['destination_ip'], axis=1, inplace=True)\n",
    "flows.drop(['external_ip'], axis=1, inplace=True)\n",
    "\n",
    "# Finally, let's inspect the outcome\n",
    "flows.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels of the dataset (as in: *Benign*, *DDoS*, *Portscan*, etc) are converted into a list of integers and split off of the main DataFrame.  \n",
    "After this step there is a variable `enc_labels` that holds an integer-encoded list of labels.\n",
    "A humble example (not representative):  \n",
    "\n",
    "|Label         | Value          |\n",
    "|------------- |---------:|\n",
    "|Benign      | 0|\n",
    "|DDoS        | 1|\n",
    "|Portscan    | 2|  \n",
    "\n",
    "So if the order of the first three Netflows would be *Benign*, *Benign*, *DDos*,  \n",
    "the resulting `enc_labels` would look like this: `[1,1,2]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T09:06:47.398200Z",
     "start_time": "2018-07-13T09:06:29.984449Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# tokenize the LABELS\n",
    "label_tokenizer = Tokenizer(num_words=20, filters='') # don't filter any of the characters. 1 entry = 1 label \n",
    "label_tokenizer.fit_on_texts(flows['label'])\n",
    "\n",
    "# Run the fitted tokenizer on the label column and save the encoded data as dataframe\n",
    "enc_labels = label_tokenizer.texts_to_sequences(flows['label'])\n",
    "enc_labels = np.concatenate(enc_labels).ravel()\n",
    "\n",
    "# as the Encoder documentation states, 0 will never assigned to a label.\n",
    "# I, on the other hand, need an index starting with 0. So we substract 1 of all classes.\n",
    "enc_labels = enc_labels -1\n",
    "\n",
    "# finally, drop the label column\n",
    "flows.drop(['label'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're at it, we make sure to never have any float values exceeding +/- infinity as well as NaN values.  \n",
    "These are all replaces by zeros, which is a temporary fix and definitely a FIXME for the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T09:07:09.162761Z",
     "start_time": "2018-07-13T09:06:47.399074Z"
    }
   },
   "outputs": [],
   "source": [
    "# weed out all NaN and infinite values\n",
    "flows.replace([np.inf, -np.inf], np.nan)\n",
    "flows.fillna(inplace=True, value=0) # FIXME: 0 for now, find a better way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Keras seems to be a bit picky about the presented datatypes, we'll convert the Pandas DataFrame into it's underlying representation of Numpy-Arrays and work with these from this point onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T09:07:14.483508Z",
     "start_time": "2018-07-13T09:07:09.163737Z"
    }
   },
   "outputs": [],
   "source": [
    "flows_nd = flows.astype('float64').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T09:07:14.816798Z",
     "start_time": "2018-07-13T09:07:14.484453Z"
    }
   },
   "outputs": [],
   "source": [
    "# as the pandas infinity stuff is seemingly not enough, check the numpy array once again\n",
    "from numpy import inf\n",
    "flows_nd[flows_nd == -inf] = 0\n",
    "flows_nd[flows_nd == inf] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T09:07:15.041278Z",
     "start_time": "2018-07-13T09:07:14.817774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has NaN: False\n",
      "Data has only finite values True\n"
     ]
    }
   ],
   "source": [
    "print(\"Data has NaN:\",np.any(np.isnan(flows_nd)))\n",
    "print(\"Data has only finite values\",np.all(np.isfinite(flows_nd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization and Finishing Touches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best performance is achieved if all values are normalized. In this approach I am using [sklearn's MinMaxScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html), which implements feature scaling through MinMax-Normalization (Rescaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T09:07:17.475939Z",
     "start_time": "2018-07-13T09:07:15.044206Z"
    }
   },
   "outputs": [],
   "source": [
    "# FIXME: Don't do normalization on test data!\n",
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "flows_scaled = min_max_scaler.fit_transform(flows_nd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a final glance at a single entry of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T09:07:17.484721Z",
     "start_time": "2018-07-13T09:07:17.479841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.22072175e-03 7.50560769e-01 3.52941176e-01 1.41666654e-07\n",
      " 4.55046005e-06 0.00000000e+00 9.30232558e-07 0.00000000e+00\n",
      " 1.13207547e-01 4.16666667e-01 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 9.99855825e-01 9.94591933e-01\n",
      " 1.66666667e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 9.99855825e-01\n",
      " 4.55046005e-06 9.32376446e-07 0.00000000e+00 0.00000000e+00\n",
      " 5.03540039e-03 0.00000000e+00 4.68259060e-06 3.58744395e-02\n",
      " 9.96078431e-01 9.80392157e-01 4.94117647e-01 7.52941176e-01\n",
      " 6.58823529e-01 3.92156863e-02 1.96078431e-02 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(flows_scaled[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T09:07:17.492529Z",
     "start_time": "2018-07-13T09:07:17.485696Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the final netflow dataset: (2830743, 47)\n",
      "Outer type: <class 'numpy.ndarray'>\n",
      "Single entry type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the final netflow dataset:\", flows_scaled.shape)\n",
    "print(\"Outer type:\", type(flows_scaled))\n",
    "print(\"Single entry type:\", type(flows_scaled[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, the [Keras Embedding Layer](https://keras.io/layers/embeddings/#embedding) expects a maximum vocabulary size, which we can simply calculate by finding max() in our scaled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T09:07:17.552079Z",
     "start_time": "2018-07-13T09:07:17.494481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum vocabulary size: 2\n"
     ]
    }
   ],
   "source": [
    "#find the maximum vocabulary size\n",
    "voc_size = (flows_scaled.max()+1).astype('int64')\n",
    "print(\"Maximum vocabulary size:\", voc_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use some nice callbacks for the model at hand. As training of LSTM nets is computationally expensive, we'll save the best model to disk.  \n",
    "Furthermore, we'll implement a callback that stops the training process as soon as the accuracy stops impproving.  \n",
    "Finally, we register the tensorboard callback, which allows for detailed insights and nice vizualizations while and after training time.\n",
    "\n",
    "**Also, this is where we define the percentages of train and test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T09:07:18.938950Z",
     "start_time": "2018-07-13T09:07:17.553040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of scaled flows: 2830743\n",
      "No of labels: 2830743\n",
      "Training Set Size: 1981520\n",
      "Validation Set Size: 849223\n"
     ]
    }
   ],
   "source": [
    "# Define some semi-global stuff\n",
    "test_size = 0.3\n",
    "batch_size = 64\n",
    "no_of_classes = len(np.unique(enc_labels))\n",
    "\n",
    "# https://stackoverflow.com/questions/3674409/how-to-split-partition-a-dataset-into-training-and-test-datasets-for-e-g-cros/18544946#18544946\n",
    "# Split training and test data, as the tensorboard embedding stuff needs embedding data, too\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(\"No of scaled flows:\", len(flows_scaled))\n",
    "print(\"No of labels:\", len(enc_labels))\n",
    "\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(flows_scaled, enc_labels, test_size=test_size, shuffle=False)\n",
    "\n",
    "print(\"Training Set Size:\",len(labels_train))\n",
    "print(\"Validation Set Size:\",len(labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T09:07:21.206925Z",
     "start_time": "2018-07-13T09:07:18.939925Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from os.path import exists, join\n",
    "run_date = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "# https://github.com/keras-team/keras/blob/master/examples/tensorboard_embeddings_mnist.py\n",
    "\n",
    "# save the class labels to disk to color data points in TensorBoard accordingly\n",
    "filename = os.path.join('logs','lstm-{}'.format(run_date),'metadata.tsv')\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'w') as f:\n",
    "    np.savetxt(f, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T09:07:23.897476Z",
     "start_time": "2018-07-13T09:07:21.208872Z"
    }
   },
   "outputs": [],
   "source": [
    "# Time for some nice vizualization stuff. Set this up and include as callback, then:\n",
    "# tensorboard --logdir=path/to/logdir\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint,TensorBoard\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='acc', # Which metric to monitor\n",
    "        patience=1     # Interrupt training after acc has stopped improving for more than 1 epoch\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath='models/lstm-{}.h5'.format(run_date),\n",
    "        monitor='val_loss',   \n",
    "        save_best_only=True    # Only save one. Only overwrite this one if val_loss has improved\n",
    "    ),\n",
    "    TensorBoard(\n",
    "        log_dir='logs/lstm-{}'.format(run_date),\n",
    "        #histogram_freq=1,     # Record activation histograms every epoch\n",
    "        #embeddings_freq=1,     # Record embedding data every epoch -> There's something wrong with the embeddings here. Keras crashed with them enabled\n",
    "        #embeddings_layer_names=['LSTMnet'],\n",
    "        #embeddings_metadata='metadata.tsv',\n",
    "        #embeddings_data=data_test,\n",
    "       # batch_size=batch_size\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T09:08:06.001226Z",
     "start_time": "2018-07-13T09:07:23.898452Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1981520 samples, validate on 849223 samples\n",
      "Epoch 1/10\n",
      " 119744/1981520 [>.............................] - ETA: 13:06 - loss: 0.4649 - acc: 0.8649"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5ca8dde60890>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                    )\n",
      "\u001b[0;32m~/Entwicklung/Master_IPython/venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Entwicklung/Master_IPython/venv/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Entwicklung/Master_IPython/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Entwicklung/Master_IPython/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Entwicklung/Master_IPython/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# see https://stackoverflow.com/a/49436133/3864726\n",
    "# This is especially important in an environment like Jupyter, where the Kernel keeps on running\n",
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(voc_size, 32)) \n",
    "model.add(LSTM(32, name='LSTMnet'))\n",
    "model.add(Dense(no_of_classes, activation='softmax')) # Multiclass classification. For binary, one would use i.e. sigmoid\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='sparse_categorical_crossentropy', # Multiclass classification! Binary would be binary_crossentropy\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(data_train, labels_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=batch_size,\n",
    "                    verbose=1,\n",
    "                    validation_data=(data_test, labels_test),\n",
    "                    callbacks=callbacks\n",
    "                   )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
